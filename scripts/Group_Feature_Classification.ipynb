{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import os\n",
    "import numpy as np\n",
    "from pytrends.request import TrendReq\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (2,6,56,65,66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw_data/all.csv')\n",
    "# Because journal rank and country rank data are only available between 2000 and 2017\n",
    "# df= df[(df['PY'] >=2000) &(df['PY'] <=2017)]    haven't decided ###############\n",
    "# fill missing key words with blank\n",
    "df['DE'] = df['DE'].fillna('')\n",
    "# concate title and key word\n",
    "df['Topic'] = df['TI'] + df['DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and filter stop words and lemmatize\n",
    "stop = set(stopwords.words('english'))\n",
    "wordnet = WordNetLemmatizer()\n",
    "wordlist = set(words.words())\n",
    "def nlp_preprocess(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    tokenized = [word for word in tokenized if word not in stop]\n",
    "    docs_lemma = ' '.join([wordnet.lemmatize(word) for word in tokenized])\n",
    "    return docs_lemma\n",
    "\n",
    "df['Topic_2'] = df['Topic'].apply(nlp_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=15, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract 15 topics\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=1000,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.Topic_2)\n",
    "\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "surface protein silk modification fibroin adsorption film chemical plasma polymer chemistry printing silica low biomaterials interaction functionalization mesh doped biocompatible\n",
      "Topic #1:\n",
      "polymer poly biodegradable membrane glycol ethylene biomaterials polymerization phase based ray process functional shape polyethylene grafting memory injectable orthopedic kinetics\n",
      "Topic #2:\n",
      "alloy titanium coating corrosion ti fiber magnesium biomaterials porous hydroxyapatite microstructure deposition behavior surface treatment ion situ plasma infection resistance\n",
      "Topic #3:\n",
      "hydrogel delivery drug synthesis chitosan release based characterization controlled polymer fabrication poly acid gelatin responsive cross nanoparticles biomaterials preparation network\n",
      "Topic #4:\n",
      "vitro biocompatibility model evaluation biological vivo study biomaterials layer metal activity antimicrobial interface cytotoxicity silver coated bio modeling biomaterial testing\n",
      "Topic #5:\n",
      "cell stem human adhesion mesenchymal differentiation culture osteoblast laser different proliferation fibroblast cellular biomaterials effect marrow control gene substrate like\n",
      "Topic #6:\n",
      "bone phosphate regeneration hydroxyapatite calcium growth factor medicine ceramic defect beta substitute repair osteogenic cement biomaterials rat formation tricalcium strategy\n",
      "Topic #7:\n",
      "acid self peptide assembly poly molecular cellulose potential copolymer modified lactic bacterial spectroscopy soft water dna assembled amino biomaterials blend\n",
      "Topic #8:\n",
      "glass bioactive gel antibacterial containing composite natural bioactivity sol based cardiac immobilization alpha enhanced binding rgd carbonate peptide mesoporous silicate\n",
      "Topic #9:\n",
      "tissue engineering scaffold collagen matrix 3d biomaterials extracellular derived cartilage based electrospun electrospinning porous chitosan engineered dimensional cancer regenerative lactide\n",
      "Topic #10:\n",
      "implant biomaterials analysis wound dental nano healing micro material dynamic device force medical particle non development processing stability technology study\n",
      "Topic #11:\n",
      "application biomedical graft biomaterials polyurethane hybrid material design endothelial thermal vascular alginate blood complex synthetic temperature adhesive small porosity strength\n",
      "Topic #12:\n",
      "biomaterials nanoparticles imaging microscopy new method electron approach high magnetic induced technique review anti using characteristic material scanning research improved\n",
      "Topic #13:\n",
      "composite response oxide polymeric platelet biomaterials macrophage reaction stress comparison inflammatory body organic enzyme inflammation activation microspheres material ionic adhesion\n",
      "Topic #14:\n",
      "property mechanical based structure carbon degradation nanotube material nerve biomaterials effect structural nanocomposites crystal neural liquid nanocomposite performance optical nanostructured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print 20 top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each documents choose the most relevant topic\n",
    "doc_topic = lda.transform(tf)\n",
    "_ = []\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    _.append(topic_most_pr)\n",
    "df['Cluster_Topic'] = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = df.groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "#Because we are predicting topic number next year\n",
    "topic_num.PY = topic_num.PY.astype(int)\n",
    "topic_num['PY'] = topic_num['PY'] +1\n",
    "topic_num.PY = topic_num.PY.astype(str)\n",
    "topic_num.Cluster_Topic = topic_num.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "topic_num['Topic_Year'] = topic_num[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "\n",
    "topic_num = topic_num[['Topic_Year','Topic_num']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg citation percent of the year per topic\n",
    "citation = df.groupby(['PY','Cluster_Topic'])['Z9'].sum().unstack().unstack().reset_index().rename(columns = {0:'Sum_Citation'})\n",
    "citation.PY = citation.PY.astype(int)\n",
    "citation.PY = citation.PY.astype(str)\n",
    "citation.Cluster_Topic = citation.Cluster_Topic.astype(str)\n",
    "citation['Topic_Year'] = citation[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "# citation = citation[['Topic_Year','Sum_Citation']]\n",
    "map_dict = citation.groupby(['PY'])['Sum_Citation'].sum().reset_index().set_index('PY').to_dict()['Sum_Citation']\n",
    "citation['Year_Sum'] = citation['PY'].map(map_dict)\n",
    "\n",
    "result = pd.merge(topic_num,citation)\n",
    "\n",
    "result['Citation_feature'] = (result['Sum_Citation']/result['Year_Sum'])/result['Topic_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['Topic_Year','Citation_feature','Topic_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(1996,2019):\n",
    "    tmp = df[df['PY'].between(year-4,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Five_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,five_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(1996,2019):\n",
    "    tmp = df[df['PY'].between(year-2,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Three_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,three_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "sjr_dir = '../data/sjrdata'\n",
    "files = os.listdir(sjr_dir)\n",
    "\n",
    "def lower_dict(d):\n",
    "    data_SJR_dict = d.set_index('Title')['SJR'].to_dict()\n",
    "    new_dict = dict((k.lower(), v) for k, v in data_SJR_dict.items())\n",
    "    return new_dict\n",
    "\n",
    "for file in files:\n",
    "    year = file.split(' ')[1].split('.')[0]\n",
    "\n",
    "    tmpdf = pd.read_csv(os.path.join(sjr_dir,file), delimiter=';',usecols = ['Title','SJR'])\n",
    "\n",
    "    df.loc[df['PY'] == int(year),'SJR'] = df['SO'].str.lower().map(lower_dict(tmpdf))\n",
    "\n",
    "df['SJR'] = df['SJR'].fillna(0)\n",
    "df['SJR'] = df['SJR'].apply(lambda x: float(str(x).replace(',','.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum SJR\n",
    "sjr = df.groupby(['PY','Cluster_Topic'])['SJR'].sum().unstack().unstack().reset_index().rename(columns = {0:'Sum_SJR'})\n",
    "sjr.PY = sjr.PY.astype(int)\n",
    "sjr.PY = sjr.PY.astype(str)\n",
    "sjr.Cluster_Topic = sjr.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "sjr['Topic_Year'] = sjr[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "sjr = sjr[['Topic_Year','Sum_SJR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(result,sjr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average SJR\n",
    "sjr = df.groupby(['PY','Cluster_Topic'])['SJR'].mean().unstack().unstack().reset_index().rename(columns = {0:'Avg_SJR'})\n",
    "sjr.PY = sjr.PY.astype(int)\n",
    "sjr.PY = sjr.PY.astype(str)\n",
    "sjr.Cluster_Topic = sjr.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "sjr['Topic_Year'] = sjr[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "sjr = sjr[['Topic_Year','Avg_SJR']]\n",
    "#merge\n",
    "result = pd.merge(result,sjr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Year'] = result['Topic_Year'].apply(lambda x:x.split('_')[1])\n",
    "result['Topic'] = result['Topic_Year'].apply(lambda x:x.split('_')[0])\n",
    "# calculate certain topic's growth rate by year\n",
    "result['Growth_Rate'] = result['Topic_num'].pct_change()\n",
    "# calculate certain topic's citation_growth_rate by year\n",
    "result['Citation_Growth_Rate'] = result['Citation_feature'].pct_change()\n",
    "#set first column to nan\n",
    "result.loc[result['Year'] == '2002','Growth_Rate'] = np.NaN\n",
    "#set first column to nan\n",
    "result.loc[result['Year'] == '2002','Citation_Growth_Rate'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate growth rate by year\n",
    "map_dict = result.groupby(['Year'])['Topic_num'].sum().pct_change().reset_index().set_index('Year').to_dict()\n",
    "\n",
    "map_dict= map_dict['Topic_num']\n",
    "\n",
    "result['Year_Growth_Rate'] = result['Year'].map(map_dict)\n",
    "\n",
    "result.loc[result['Year'] == '2002','Year_Growth_Rate'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if topic's growth rate is larger than the growth rate of that year then set target to 1 else 0\n",
    "result['Target'] = result['Growth_Rate']  > result['Year_Growth_Rate']\n",
    "\n",
    "result['Target'] =result.Target.apply(lambda x: int(x==True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift feature by year\n",
    "def shift_col_by_year(col,year,df):\n",
    "    new_col = '_'.join([col,str(year)])\n",
    "    df[new_col] = df[col].shift(year)\n",
    "    for i in range(2002,2002+year):\n",
    "        df.loc[df['Year'] == str(i),new_col] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_col_by_year('Avg_SJR',1,result)\n",
    "shift_col_by_year('Avg_SJR',2,result)\n",
    "shift_col_by_year('Avg_SJR',3,result)\n",
    "shift_col_by_year('Sum_SJR',1,result)\n",
    "shift_col_by_year('Sum_SJR',2,result)\n",
    "shift_col_by_year('Sum_SJR',3,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trend feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_most_k_common(topic_num,k):\n",
    "    tmp = df[df['Cluster_Topic'] == topic_num]\n",
    "    cunter = Counter([i.strip().lower() for i in ';'.join(tmp.DE.values).split(';') if i]).most_common(k)\n",
    "    return [i[0] for i in cunter if i[0] != 'biomaterials' and i[0] != 'biomaterial']\n",
    "\n",
    "keyword_dict = {}\n",
    "for topic_num in range(15):\n",
    "    keyword_dict[topic_num] = calculate_topic_most_k_common(topic_num,10)[:4] #only save 4 words each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_trend(topic,keyword_dict):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "    #get google trends for 5 words for each topic. 5 words contain 4 top key words and 'biomaterial' as benchmark\n",
    "    kw_list = keyword_dict[topic] + ['biomaterials'] #############\n",
    "    pytrends.build_payload(kw_list, timeframe='2004-01-01 2018-12-31')\n",
    "    # Interest Over Time\n",
    "    key_trends = pytrends.interest_over_time().reset_index()\n",
    "    key_trends['Year'] = key_trends['date'].apply(lambda x:x.year)\n",
    "    key_trends = key_trends[kw_list + ['Year']] #only keep words columns\n",
    "    key_trends['Topic_trend'] = key_trends[keyword_dict[topic]].sum(axis = 1) #sum up 4 key words's trends \n",
    "    tmp = (key_trends.groupby('Year')['Topic_trend'].sum() / key_trends.groupby('Year')['biomaterials'].sum()) \\\n",
    "        .reset_index().rename(columns = {0:'Google Trend','Year':'Topic_Year'})\n",
    "    tmp['Topic_Year'] = tmp['Topic_Year'].apply(lambda x: '_'.join([str(topic),str(x)])) ####\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmplist = []\n",
    "for i in range(15):\n",
    "    tmplist.append(get_topic_trend(i,keyword_dict))\n",
    "google_trend = pd.concat(tmplist,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of reviews each year per topic\n",
    "reviews = df[df['DT'] == 'Review']\n",
    "reviews = reviews.groupby(['PY','Cluster_Topic'])['DT'].count().unstack().unstack().reset_index().rename(columns = {0:'Review_AMT'})\n",
    "reviews.PY = reviews.PY.astype(int)\n",
    "reviews.PY = reviews.PY.astype(str)\n",
    "reviews.Cluster_Topic = reviews.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "reviews['Topic_Year'] = reviews[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "reviews = reviews[['Topic_Year','Review_AMT']]\n",
    "reviews = reviews.fillna(0)\n",
    "\n",
    "result = pd.merge(result,reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../output/result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Year</th>\n",
       "      <th>Citation_feature</th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Five_Year_Percent</th>\n",
       "      <th>Three_Year_Percent</th>\n",
       "      <th>Sum_SJR</th>\n",
       "      <th>Avg_SJR</th>\n",
       "      <th>Year</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Growth_Rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Year_Growth_Rate</th>\n",
       "      <th>Target</th>\n",
       "      <th>Avg_SJR_1</th>\n",
       "      <th>Avg_SJR_2</th>\n",
       "      <th>Avg_SJR_3</th>\n",
       "      <th>Sum_SJR_1</th>\n",
       "      <th>Sum_SJR_2</th>\n",
       "      <th>Sum_SJR_3</th>\n",
       "      <th>Google Trend</th>\n",
       "      <th>Review_AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_2004</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.245223</td>\n",
       "      <td>0.351598</td>\n",
       "      <td>62.657</td>\n",
       "      <td>0.813727</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.054795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0</td>\n",
       "      <td>1.020058</td>\n",
       "      <td>1.056329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.384</td>\n",
       "      <td>77.112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_2005</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.259459</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>94.646</td>\n",
       "      <td>0.985896</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173494</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813727</td>\n",
       "      <td>1.020058</td>\n",
       "      <td>1.056329</td>\n",
       "      <td>62.657</td>\n",
       "      <td>70.384</td>\n",
       "      <td>77.112</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_2006</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.316206</td>\n",
       "      <td>78.860</td>\n",
       "      <td>0.985750</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985896</td>\n",
       "      <td>0.813727</td>\n",
       "      <td>1.020058</td>\n",
       "      <td>94.646</td>\n",
       "      <td>62.657</td>\n",
       "      <td>70.384</td>\n",
       "      <td>0.395802</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_2007</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.329167</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>183.868</td>\n",
       "      <td>1.163722</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136501</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985750</td>\n",
       "      <td>0.985896</td>\n",
       "      <td>0.813727</td>\n",
       "      <td>78.860</td>\n",
       "      <td>94.646</td>\n",
       "      <td>62.657</td>\n",
       "      <td>0.424408</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_2008</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.365333</td>\n",
       "      <td>163.017</td>\n",
       "      <td>1.189905</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461135</td>\n",
       "      <td>1</td>\n",
       "      <td>1.163722</td>\n",
       "      <td>0.985750</td>\n",
       "      <td>0.985896</td>\n",
       "      <td>183.868</td>\n",
       "      <td>78.860</td>\n",
       "      <td>94.646</td>\n",
       "      <td>0.379377</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic_Year  Citation_feature  Topic_num  Five_Year_Percent  \\\n",
       "0     0_2004          0.001447       69.0           0.245223   \n",
       "1     0_2005          0.001029       77.0           0.259459   \n",
       "2     0_2006          0.000724       96.0           0.202532   \n",
       "3     0_2007          0.001357       80.0           0.329167   \n",
       "4     0_2008          0.000527      158.0           0.250000   \n",
       "\n",
       "   Three_Year_Percent  Sum_SJR   Avg_SJR  Year Topic  Growth_Rate     ...      \\\n",
       "0            0.351598   62.657  0.813727  2004     0    -0.054795     ...       \n",
       "1            0.396694   94.646  0.985896  2005     0     0.115942     ...       \n",
       "2            0.316206   78.860  0.985750  2006     0     0.246753     ...       \n",
       "3            0.473054  183.868  1.163722  2007     0    -0.166667     ...       \n",
       "4            0.365333  163.017  1.189905  2008     0     0.975000     ...       \n",
       "\n",
       "   Year_Growth_Rate  Target  Avg_SJR_1  Avg_SJR_2  Avg_SJR_3  Sum_SJR_1  \\\n",
       "0          0.160839       0   1.020058   1.056329        NaN     70.384   \n",
       "1          0.173494       0   0.813727   1.020058   1.056329     62.657   \n",
       "2          0.361396       0   0.985896   0.813727   1.020058     94.646   \n",
       "3         -0.136501       0   0.985750   0.985896   0.813727     78.860   \n",
       "4          0.461135       1   1.163722   0.985750   0.985896    183.868   \n",
       "\n",
       "   Sum_SJR_2  Sum_SJR_3  Google Trend  Review_AMT  \n",
       "0     77.112        NaN      0.480263         4.0  \n",
       "1     70.384     77.112      0.362663         5.0  \n",
       "2     62.657     70.384      0.395802        13.0  \n",
       "3     94.646     62.657      0.424408        13.0  \n",
       "4     78.860     94.646      0.379377        14.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
