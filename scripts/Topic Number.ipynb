{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (2,6,56,65,66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw_data/all.csv')\n",
    "# Because journal rank and country rank data are only available between 2000 and 2017\n",
    "# df= df[(df['PY'] >=2000) &(df['PY'] <=2017)]    haven't decided ###############\n",
    "# fill missing key words with blank\n",
    "df['DE'] = df['DE'].fillna('')\n",
    "# concate title and key word\n",
    "df['Topic'] = df['TI'] + df['DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and filter stop words and lemmatize\n",
    "stop = set(stopwords.words('english'))\n",
    "wordnet = WordNetLemmatizer()\n",
    "wordlist = set(words.words())\n",
    "def nlp_preprocess(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    tokenized = [word for word in tokenized if word not in stop]\n",
    "    docs_lemma = ' '.join([wordnet.lemmatize(word) for word in tokenized])\n",
    "    return docs_lemma\n",
    "\n",
    "df['Topic_2'] = df['Topic'].apply(nlp_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=15, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract 15 topics\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=1000,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.Topic_2)\n",
    "\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "surface poly porous modification adhesion adsorption protein plasma treatment polymer chemical modified glycol ethylene bacterial coating caprolactone titanium epsilon functionalization\n",
      "Topic #1:\n",
      "model fiber polymerization induced printing using polyester biomaterials steel stainless small animal bioprinting disease nanostructures nanomaterials fluorescence long simulation migration\n",
      "Topic #2:\n",
      "hydroxyapatite phosphate bone calcium composite glass bioactive ceramic alginate formation nerve natural bioactivity cement ph crystal tricalcium apatite sintering fracture\n",
      "Topic #3:\n",
      "film laser microscopy activity spectroscopy chemistry deposition body fibroblast force morphology electron cancer biomaterials polysaccharide alcohol vinyl optical ray enzyme\n",
      "Topic #4:\n",
      "property mechanical alloy corrosion coating biomaterials behavior titanium ti polyurethane composite effect antibacterial magnesium thermal based microstructure biomedical material nanocomposites\n",
      "Topic #5:\n",
      "chitosan vitro osteoblast cellulose biological gelatin endothelial free infection graphene biomaterial blend biomaterials role comparison biofilm reconstruction evaluation investigation material\n",
      "Topic #6:\n",
      "biomaterials biodegradable gel based cartilage development poly approach interface reaction sol polymer technique lactide liquid material performance novel surgery fibrin\n",
      "Topic #7:\n",
      "tissue engineering scaffold silk matrix bone fibroin regeneration 3d biomaterials graft extracellular electrospun electrospinning engineered based repair review cardiac vascular\n",
      "Topic #8:\n",
      "cell stem human mesenchymal growth differentiation derived culture factor healing regenerative bone medicine biomaterials adhesion proliferation wound osteogenic skin matrix\n",
      "Topic #9:\n",
      "bone collagen implant study biomaterials vivo new vitro dental nano beta analysis imaging defect biocompatibility regeneration material micro medical degradation\n",
      "Topic #10:\n",
      "oxide nanoparticles carbon membrane fabrication blood layer biomaterials platelet phase nanotube antimicrobial potential water silver polyethylene biocompatible plasma compatibility testing\n",
      "Topic #11:\n",
      "acid structure high molecular method interaction biomaterials lactic dynamic cellular soft hyaluronic structural amino protein temperature using light microspheres ionic\n",
      "Topic #12:\n",
      "hydrogel delivery drug release controlled biomaterials functional polymeric like device polymer nanoparticles situ injectable supramolecular complex anti dna therapy gene\n",
      "Topic #13:\n",
      "self polymer protein synthesis peptide application assembly biomaterials based characterization design biomedical biomimetic material copolymer responsive hybrid preparation network nanofibers\n",
      "Topic #14:\n",
      "response metal ion dimensional cross macrophage particle magnetic silica wound stability inflammatory process production binding inflammation bio linking prepared time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print 20 top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each documents choose the most relevant topic\n",
    "doc_topic = lda.transform(tf)\n",
    "_ = []\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    _.append(topic_most_pr)\n",
    "df['Cluster_Topic'] = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = df.groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "citation = df.groupby(['PY','Cluster_Topic'])['Z9'].sum().unstack().unstack().reset_index().rename(columns = {0:'Sum_Citation'})\n",
    "#Because we are predicting topic number next year\n",
    "topic_num.PY = topic_num.PY.astype(int)\n",
    "topic_num['PY'] = topic_num['PY'] +1\n",
    "topic_num.PY = topic_num.PY.astype(str)\n",
    "topic_num.Cluster_Topic = topic_num.Cluster_Topic.astype(str)\n",
    "citation.PY = citation.PY.astype(int)\n",
    "citation.PY = citation.PY.astype(str)\n",
    "citation.Cluster_Topic = citation.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "topic_num['Topic_Year'] = topic_num[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "citation['Topic_Year'] = citation[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "topic_num = topic_num[['Topic_Year','Topic_num']]\n",
    "citation = citation[['Topic_Year','Sum_Citation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(topic_num,citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(2001,2019):\n",
    "    tmp = df[df['PY'].between(year-4,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp.groupby(['Cluster_Topic'])['Topic_num'].sum()\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Five_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,five_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(2001,2019):\n",
    "    tmp = df[df['PY'].between(year-2,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp.groupby(['Cluster_Topic'])['Topic_num'].sum()\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Three_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,three_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Year</th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Sum_Citation</th>\n",
       "      <th>Five_Year_Percent</th>\n",
       "      <th>Three_Year_Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_2002</td>\n",
       "      <td>72.0</td>\n",
       "      <td>7219.0</td>\n",
       "      <td>0.341549</td>\n",
       "      <td>0.573964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_2003</td>\n",
       "      <td>97.0</td>\n",
       "      <td>6510.0</td>\n",
       "      <td>0.332353</td>\n",
       "      <td>0.400709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_2004</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6138.0</td>\n",
       "      <td>0.296758</td>\n",
       "      <td>0.361702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_2005</td>\n",
       "      <td>119.0</td>\n",
       "      <td>7152.0</td>\n",
       "      <td>0.251866</td>\n",
       "      <td>0.367847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_2006</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4255.0</td>\n",
       "      <td>0.222781</td>\n",
       "      <td>0.343669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic_Year  Topic_num  Sum_Citation  Five_Year_Percent  Three_Year_Percent\n",
       "0     0_2002       72.0        7219.0           0.341549            0.573964\n",
       "1     0_2003       97.0        6510.0           0.332353            0.400709\n",
       "2     0_2004      113.0        6138.0           0.296758            0.361702\n",
       "3     0_2005      119.0        7152.0           0.251866            0.367847\n",
       "4     0_2006      135.0        4255.0           0.222781            0.343669"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
