{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (2,6,56,65,66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw_data/all.csv')\n",
    "# Because journal rank and country rank data are only available between 2000 and 2017\n",
    "# df= df[(df['PY'] >=2000) &(df['PY'] <=2017)]    haven't decided ###############\n",
    "# fill missing key words with blank\n",
    "df['DE'] = df['DE'].fillna('')\n",
    "# concate title and key word\n",
    "df['Topic'] = df['TI'] + df['DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and filter stop words and lemmatize\n",
    "stop = set(stopwords.words('english'))\n",
    "wordnet = WordNetLemmatizer()\n",
    "wordlist = set(words.words())\n",
    "def nlp_preprocess(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    tokenized = [word for word in tokenized if word not in stop]\n",
    "    docs_lemma = ' '.join([wordnet.lemmatize(word) for word in tokenized])\n",
    "    return docs_lemma\n",
    "\n",
    "df['Topic_2'] = df['Topic'].apply(nlp_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=15, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract 15 topics\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=1000,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.Topic_2)\n",
    "\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "surface protein silk modification fibroin adsorption film chemical plasma polymer chemistry printing silica low biomaterials interaction functionalization mesh doped biocompatible\n",
      "Topic #1:\n",
      "polymer poly biodegradable membrane glycol ethylene biomaterials polymerization phase based ray process functional shape polyethylene grafting memory injectable orthopedic kinetics\n",
      "Topic #2:\n",
      "alloy titanium coating corrosion ti fiber magnesium biomaterials porous hydroxyapatite microstructure deposition behavior surface treatment ion situ plasma infection resistance\n",
      "Topic #3:\n",
      "hydrogel delivery drug synthesis chitosan release based characterization controlled polymer fabrication poly acid gelatin responsive cross nanoparticles biomaterials preparation network\n",
      "Topic #4:\n",
      "vitro biocompatibility model evaluation biological vivo study biomaterials layer metal activity antimicrobial interface cytotoxicity silver coated bio modeling biomaterial testing\n",
      "Topic #5:\n",
      "cell stem human adhesion mesenchymal differentiation culture osteoblast laser different proliferation fibroblast cellular biomaterials effect marrow control gene substrate like\n",
      "Topic #6:\n",
      "bone phosphate regeneration hydroxyapatite calcium growth factor medicine ceramic defect beta substitute repair osteogenic cement biomaterials rat formation tricalcium strategy\n",
      "Topic #7:\n",
      "acid self peptide assembly poly molecular cellulose potential copolymer modified lactic bacterial spectroscopy soft water dna assembled amino biomaterials blend\n",
      "Topic #8:\n",
      "glass bioactive gel antibacterial containing composite natural bioactivity sol based cardiac immobilization alpha enhanced binding rgd carbonate peptide mesoporous silicate\n",
      "Topic #9:\n",
      "tissue engineering scaffold collagen matrix 3d biomaterials extracellular derived cartilage based electrospun electrospinning porous chitosan engineered dimensional cancer regenerative lactide\n",
      "Topic #10:\n",
      "implant biomaterials analysis wound dental nano healing micro material dynamic device force medical particle non development processing stability technology study\n",
      "Topic #11:\n",
      "application biomedical graft biomaterials polyurethane hybrid material design endothelial thermal vascular alginate blood complex synthetic temperature adhesive small porosity strength\n",
      "Topic #12:\n",
      "biomaterials nanoparticles imaging microscopy new method electron approach high magnetic induced technique review anti using characteristic material scanning research improved\n",
      "Topic #13:\n",
      "composite response oxide polymeric platelet biomaterials macrophage reaction stress comparison inflammatory body organic enzyme inflammation activation microspheres material ionic adhesion\n",
      "Topic #14:\n",
      "property mechanical based structure carbon degradation nanotube material nerve biomaterials effect structural nanocomposites crystal neural liquid nanocomposite performance optical nanostructured\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print 20 top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each documents choose the most relevant topic\n",
    "doc_topic = lda.transform(tf)\n",
    "_ = []\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    _.append(topic_most_pr)\n",
    "df['Cluster_Topic'] = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = df.groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "#Because we are predicting topic number next year\n",
    "topic_num.PY = topic_num.PY.astype(int)\n",
    "topic_num['PY'] = topic_num['PY'] +1\n",
    "topic_num.PY = topic_num.PY.astype(str)\n",
    "topic_num.Cluster_Topic = topic_num.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "topic_num['Topic_Year'] = topic_num[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "\n",
    "topic_num = topic_num[['Topic_Year','Topic_num']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg citation percent of the year per topic\n",
    "citation = df.groupby(['PY','Cluster_Topic'])['Z9'].sum().unstack().unstack().reset_index().rename(columns = {0:'Sum_Citation'})\n",
    "citation.PY = citation.PY.astype(int)\n",
    "citation.PY = citation.PY.astype(str)\n",
    "citation.Cluster_Topic = citation.Cluster_Topic.astype(str)\n",
    "citation['Topic_Year'] = citation[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "# citation = citation[['Topic_Year','Sum_Citation']]\n",
    "map_dict = citation.groupby(['PY'])['Sum_Citation'].sum().reset_index().set_index('PY').to_dict()['Sum_Citation']\n",
    "citation['Year_Sum'] = citation['PY'].map(map_dict)\n",
    "\n",
    "result = pd.merge(topic_num,citation)\n",
    "\n",
    "result['Citation_feature'] = (result['Sum_Citation']/result['Year_Sum'])/result['Topic_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['Topic_Year','Citation_feature','Topic_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(1996,2019):\n",
    "    tmp = df[df['PY'].between(year-4,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Five_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,five_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each topic in each year, calculate the ratio of number of paper\n",
    "#published this year to the number of paper published in latest five years\n",
    "tmplist = []\n",
    "for year in range(1996,2019):\n",
    "    tmp = df[df['PY'].between(year-2,year)].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmp2 = df[df['PY'] == year].groupby(['PY'])['Cluster_Topic'].value_counts().unstack().unstack().reset_index().rename(columns = {0:'Topic_num'})\n",
    "\n",
    "    tmpdf = tmp2['Topic_num']/tmp.groupby(['Cluster_Topic'])['Topic_num'].sum().tolist()\n",
    "\n",
    "    year = year\n",
    "    tmpyear = [str(i) + '_' + str(year) for i in range(15)]\n",
    "\n",
    "    tmplist.append(pd.DataFrame({'Topic_Year':tmpyear,'Three_Year_Percent':tmpdf}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_year_percent = pd.concat(tmplist,ignore_index=True)\n",
    "\n",
    "result = pd.merge(result,three_year_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "sjr_dir = '../data/sjrdata'\n",
    "files = os.listdir(sjr_dir)\n",
    "\n",
    "def lower_dict(d):\n",
    "    data_SJR_dict = d.set_index('Title')['SJR'].to_dict()\n",
    "    new_dict = dict((k.lower(), v) for k, v in data_SJR_dict.items())\n",
    "    return new_dict\n",
    "\n",
    "for file in files:\n",
    "    year = file.split(' ')[1].split('.')[0]\n",
    "\n",
    "    tmpdf = pd.read_csv(os.path.join(sjr_dir,file), delimiter=';',usecols = ['Title','SJR'])\n",
    "\n",
    "    df.loc[df['PY'] == int(year),'SJR'] = df['SO'].str.lower().map(lower_dict(tmpdf))\n",
    "\n",
    "df['SJR'] = df['SJR'].fillna(0)\n",
    "df['SJR'] = df['SJR'].apply(lambda x: float(str(x).replace(',','.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum SJR\n",
    "sjr = df.groupby(['PY','Cluster_Topic'])['SJR'].sum().unstack().unstack().reset_index().rename(columns = {0:'Sum_SJR'})\n",
    "sjr.PY = sjr.PY.astype(int)\n",
    "sjr.PY = sjr.PY.astype(str)\n",
    "sjr.Cluster_Topic = sjr.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "sjr['Topic_Year'] = sjr[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "sjr = sjr[['Topic_Year','Sum_SJR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(result,sjr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average SJR\n",
    "sjr = df.groupby(['PY','Cluster_Topic'])['SJR'].mean().unstack().unstack().reset_index().rename(columns = {0:'Avg_SJR'})\n",
    "sjr.PY = sjr.PY.astype(int)\n",
    "sjr.PY = sjr.PY.astype(str)\n",
    "sjr.Cluster_Topic = sjr.Cluster_Topic.astype(str)\n",
    "# create key for join format: topic_year\n",
    "sjr['Topic_Year'] = sjr[['Cluster_Topic','PY']].apply(lambda x: '_'.join(x),axis =1)\n",
    "sjr = sjr[['Topic_Year','Avg_SJR']]\n",
    "#merge\n",
    "result = pd.merge(result,sjr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Year'] = result['Topic_Year'].apply(lambda x:x.split('_')[1])\n",
    "result['Topic'] = result['Topic_Year'].apply(lambda x:x.split('_')[0])\n",
    "# calculate certain topic's growth rate by year\n",
    "result['Growth_Rate'] = result['Topic_num'].pct_change()\n",
    "# calculate certain topic's citation_growth_rate by year\n",
    "result['Citation_Growth_Rate'] = result['Citation_feature'].pct_change()\n",
    "#set first column to nan\n",
    "result.loc[result['Year'] == '2002','Growth_Rate'] = np.NaN\n",
    "#set first column to nan\n",
    "result.loc[result['Year'] == '2002','Citation_Growth_Rate'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate growth rate by year\n",
    "map_dict = result.groupby(['Year'])['Topic_num'].sum().pct_change().reset_index().set_index('Year').to_dict()\n",
    "\n",
    "map_dict= map_dict['Topic_num']\n",
    "\n",
    "result['Year_Growth_Rate'] = result['Year'].map(map_dict)\n",
    "\n",
    "result.loc[result['Year'] == '2002','Year_Growth_Rate'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if topic's growth rate is larger than the growth rate of that year then set target to 1 else 0\n",
    "result['Target'] = result['Growth_Rate']  > result['Year_Growth_Rate']\n",
    "\n",
    "result['Target'] =result.Target.apply(lambda x: int(x==True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift feature by year\n",
    "def shift_col_by_year(col,year,df):\n",
    "    new_col = '_'.join([col,str(year)])\n",
    "    df[new_col] = df[col].shift(year)\n",
    "    for i in range(2002,2002+year):\n",
    "        df.loc[df['Year'] == str(i),new_col] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_col_by_year('Avg_SJR',1,result)\n",
    "shift_col_by_year('Avg_SJR',2,result)\n",
    "shift_col_by_year('Avg_SJR',3,result)\n",
    "shift_col_by_year('Sum_SJR',1,result)\n",
    "shift_col_by_year('Sum_SJR',2,result)\n",
    "shift_col_by_year('Sum_SJR',3,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../output/result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Year</th>\n",
       "      <th>Citation_feature</th>\n",
       "      <th>Topic_num</th>\n",
       "      <th>Five_Year_Percent</th>\n",
       "      <th>Three_Year_Percent</th>\n",
       "      <th>Sum_SJR</th>\n",
       "      <th>Avg_SJR</th>\n",
       "      <th>Year</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Growth_Rate</th>\n",
       "      <th>Citation_Growth_Rate</th>\n",
       "      <th>Year_Growth_Rate</th>\n",
       "      <th>Target</th>\n",
       "      <th>Avg_SJR_1</th>\n",
       "      <th>Avg_SJR_2</th>\n",
       "      <th>Avg_SJR_3</th>\n",
       "      <th>Sum_SJR_1</th>\n",
       "      <th>Sum_SJR_2</th>\n",
       "      <th>Sum_SJR_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_1996</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_1997</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.234483</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>-0.652700</td>\n",
       "      <td>0.180272</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_1998</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.225610</td>\n",
       "      <td>0.324561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.209302</td>\n",
       "      <td>0.592603</td>\n",
       "      <td>0.083573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_1999</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>37.140</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_2000</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.341880</td>\n",
       "      <td>45.009</td>\n",
       "      <td>1.125225</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>-0.445003</td>\n",
       "      <td>0.100446</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic_Year  Citation_feature  Topic_num  Five_Year_Percent  \\\n",
       "0     0_1996          0.006168       25.0           0.338583   \n",
       "1     0_1997          0.002142       43.0           0.234483   \n",
       "2     0_1998          0.003411       34.0           0.225610   \n",
       "3     0_1999          0.003360       37.0           0.223464   \n",
       "4     0_2000          0.001865       40.0           0.206186   \n",
       "\n",
       "   Three_Year_Percent  Sum_SJR   Avg_SJR  Year Topic  Growth_Rate  \\\n",
       "0            0.462366    0.000  0.000000  1996     0          NaN   \n",
       "1            0.333333    0.000  0.000000  1997     0     0.720000   \n",
       "2            0.324561    0.000  0.000000  1998     0    -0.209302   \n",
       "3            0.360360   37.140  0.928500  1999     0     0.088235   \n",
       "4            0.341880   45.009  1.125225  2000     0     0.081081   \n",
       "\n",
       "   Citation_Growth_Rate  Year_Growth_Rate  Target  Avg_SJR_1  Avg_SJR_2  \\\n",
       "0                   NaN               NaN       0        NaN        NaN   \n",
       "1             -0.652700          0.180272       1     0.0000        NaN   \n",
       "2              0.592603          0.083573       0     0.0000        0.0   \n",
       "3             -0.015133          0.191489       0     0.0000        0.0   \n",
       "4             -0.445003          0.100446       0     0.9285        0.0   \n",
       "\n",
       "   Avg_SJR_3  Sum_SJR_1  Sum_SJR_2  Sum_SJR_3  \n",
       "0        NaN        NaN        NaN        NaN  \n",
       "1        NaN       0.00        NaN        NaN  \n",
       "2        NaN       0.00        0.0        NaN  \n",
       "3        0.0       0.00        0.0        0.0  \n",
       "4        0.0      37.14        0.0        0.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
